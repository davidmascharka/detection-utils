{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This notebook will walk through training a simple single-stage object detector from scratch using\n",
    "the `detection-utils` library. The detector you'll build here is based on the recent [RetinaNet in\n",
    "the paper \"Focal Loss for Dense Object Detection\"](https://arxiv.org/abs/1708.02002). This notebook\n",
    "will assume high-level familiarity with the problem of object detection and is not intended to be a\n",
    "full tutorial for the problem. Rather, this is intended to be a full working example of using\n",
    "`detection-utils` to construct an object detector. That said, a good amount of exposition is\n",
    "provided, which can serve as an accelerated introduction to object detection.\n",
    "\n",
    "The first step is to generate a dataset. We'll construct a simple dataset of 2d shapes on a\n",
    "background of colored noise by using the `generate-dataset.py` script in this directory. You can run\n",
    "this from the command line or just execute the cell below to generate 1000 training and 200\n",
    "validation images. Feel free to tweak those numbers. Basic usage of the script is:\n",
    "\n",
    "``` shell\n",
    "$ python generate-dataset.py -d /path/to/destination -n number_of_images\n",
    "```\n",
    "\n",
    "For additional options, see `python generate-dataset.py -h` to get help output. If this takes more\n",
    "than about 20-30 seconds, kill it and restart. The script is a bit brittle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!python generate-dataset.py -d data/train -n 1000\n",
    "!python generate-dataset.py -d data/val -n 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You should now have 1000 training and 200 validation images stored in `./data/train` and\n",
    "`./data/val`, respectively. Each of the images will have between 3 and 8 objects per image and each\n",
    "of the objects will be between 24 and 40 pixels in each dimension. Each of the images is 256x256 and\n",
    "stored as RGB. We have three data files for each of our training and validation sets:\n",
    "\n",
    "- `images.npy` is a `float32` array of shape `(N, 256, 256, 3)`, where `N` is the number of images.\n",
    "- `boxes.npy` is an `object` array of shape `(N,)`. Each of the elements is an `int32` array of \n",
    "   shape `(K, 4)`, where `K` is the number of objects in that image.\n",
    "- `labels.npy`is an `object` array of shape `(N,)`. Each of the elements is an `int32` array of\n",
    "  shape `(K,)`, where `K` is the\n",
    "- number of objects in that image.\n",
    "\n",
    "Our `boxes` array stores boxes in `(left, top, right, bottom)` format, which we refer to as\n",
    "`xyxy`. We'll discuss the data in more detail when we get to it. For now, let's cover our basic\n",
    "imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from detection_utils.boxes import non_max_suppression, generate_targets\n",
    "from detection_utils.metrics import compute_recall, compute_precision\n",
    "from detection_utils.pytorch import softmax_focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We'll start out by defining a few constants. First up is our batch size. We'll use a batch size of\n",
    "16 here. Feel free to change this as needed for GPU memory or as desired. Choice of batch size is\n",
    "not too important for training.\n",
    "\n",
    "We'll also set the device to use for training. Valid options here are `'cuda'`, `'cuda:N'` where `N`\n",
    "is the index of the GPU on your machine that you want to use, and `'cpu'`. If you have a GPU\n",
    "available, we certainly recommend utilizing it. However, training this simple model on CPU is\n",
    "possible; it won't take *too* much time to get a decent level of performance. If you have multiple\n",
    "GPUs, we recommend choosing a GPU that you are not rendering your display on. Using your display GPU\n",
    "for compute can cause some lockups and rendering issues/delays. Below, we would ordinarily change\n",
    "`'cuda:0'` to a non-zero GPU, but we chose 0 to work out of the box for more people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16  # feel free to manipulate this as needed for GPU memory or as desired\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# here we'll define a simple helper function to load a dataset\n",
    "# then load our training and validation sets\n",
    "def load_data(root_dir):\n",
    "    root_dir = Path(root_dir)\n",
    "    images = np.load(root_dir / 'images.npy')\n",
    "    # we're loading object arrays here so we need to allow_pickle\n",
    "    boxes = np.load(root_dir / 'boxes.npy', allow_pickle=True)\n",
    "    labels = np.load(root_dir / 'labels.npy', allow_pickle=True)\n",
    "    return images, boxes, labels\n",
    "\n",
    "train_images, train_boxes, train_labels = load_data('data/train/')\n",
    "val_images, val_boxes, val_labels = load_data('data/val/')\n",
    "\n",
    "# for performance, we'll load all of our images onto our device ahead of time to minimize\n",
    "# data transfers from CPU -> GPU. We know we're working with a small dataset, so this shouldn't\n",
    "# cause problems. This uses about a gigabyte of GPU memory.\n",
    "# We'll do the transpose to convert our images from shape (N, 256, 256, 3) to (N, 3, 256, 256)\n",
    "# which is the ordering we need for our CNN.\n",
    "train_images = torch.tensor(train_images.transpose(0, 3, 1, 2)).to(device)\n",
    "val_images = torch.tensor(val_images.transpose(0, 3, 1, 2)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now we'll create our model. As mentioned previously, we're using an architecture based on\n",
    "RetinaNet. Using a feature pyramid network would be incredible overkill, so we don't incorporate an\n",
    "FPN. We end up with a very simple backbone network followed by simple classification and regression\n",
    "heads. Feel free to tweak the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(20, 30, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(30, 40, 3, padding=1)\n",
    "        \n",
    "        self.classification = nn.Conv2d(40, 4, 1) # background / rectangle / triangle / circle\n",
    "        self.regression = nn.Conv2d(40, 4, 1)\n",
    "        \n",
    "        for layer in (self.conv1, self.conv2, self.conv3, self.conv4,\n",
    "                     self.classification, self.regression):\n",
    "            nn.init.xavier_normal_(layer.weight, np.sqrt(2))\n",
    "            nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "        nn.init.constant_(self.classification.bias[0], -4.6)  # rougly -log((1-π)/π) for π = 0.01\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv4(x)), 2)\n",
    "        \n",
    "        classifications = self.classification(x).permute(0, 2, 3, 1)                          # (N, R, C, # classes)\n",
    "        classifications = classifications.reshape(x.shape[0], -1, classifications.shape[-1])  # (N, R*C, # classes)\n",
    "        regressions = self.regression(x).permute(0, 2, 3, 1)                                  # (N, R, C, # classes)\n",
    "        regressions = regressions.reshape(x.shape[0], -1, 4)                                  # (N, R*C, 4)\n",
    "        return classifications, regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Next we'll need a set of anchor boxes for our detector. We know a priori that our objects are sized\n",
    "in [24, 40] so we can pick an anchor box size somewhere around there. We'll go right in the middle\n",
    "with an anchor box size of 32x32. This is a bit of a cheat, but simplifies the example and is\n",
    "comparable to the common practice of clustering the box sizes in your training dataset. Our model\n",
    "downsamples the image by a factor of 16, so we'll stride our anchors across the images with a step\n",
    "of 16. This ensures that each entry in the feature map corresponds to the center of an anchor box \n",
    "in pixel space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "anchor_boxes = []\n",
    "for y in range(0, train_images.shape[2], 16):\n",
    "    for x in range(0, train_images.shape[3], 16):\n",
    "        anchor_boxes.append(np.array([-16, -16, 16, 16]) + np.array([x, y, x, y]))\n",
    "anchor_boxes = np.vstack(anchor_boxes)\n",
    "# anchor_boxes are in xyxy format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now that we have our anchor boxes, we need to generate a set of targets for training. What we're\n",
    "computing here is which anchor box(es) are responsible for each object in the training set. For each\n",
    "anchor box responsible for an object, we determine the regression offsets needed to shift the anchor\n",
    "to cover the object. We'll also figure out the corresponding label.\n",
    "\n",
    "The real benefit of this is that all of our target lists are going to be the same shape (because the\n",
    "number of anchor boxes is the same) regardless of how many objects are in each scene. This allows us\n",
    "to batch processing, which we wouldn't be able to do otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_cls_targs, train_reg_targs = zip(*(generate_targets(anchor_boxes, bxs, lbls, 0.2, 0.1)\n",
    "                                         for bxs, lbls in zip(train_boxes, train_labels)))\n",
    "val_cls_targs, val_reg_targs = zip(*(generate_targets(anchor_boxes, bxs, lbls, 0.2, 0.1) \n",
    "                                     for bxs, lbls in zip(val_boxes, val_labels)))\n",
    "\n",
    "# For convenience (and to minimize data transfers) we'll shove all of this onto our\n",
    "# device as well. This only takes a couple of megabytes of GPU memory\n",
    "train_reg_targs = torch.tensor(train_reg_targs).float().to(device)\n",
    "train_cls_targs = torch.tensor(train_cls_targs).long().to(device)\n",
    "val_reg_targs = torch.tensor(val_reg_targs).float().to(device)\n",
    "val_cls_targs = torch.tensor(val_cls_targs).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our loss function is a combination of focal loss for classification\n",
    "# and smooth L1 (Huber) loss for regression\n",
    "def loss(class_predictions, regression_predictions, class_targets, regression_targets):\n",
    "    class_targets = class_targets.reshape(-1)\n",
    "    regression_targets = regression_targets.reshape(-1, 4)\n",
    "    class_predictions = class_predictions.reshape(-1, class_predictions.shape[-1])\n",
    "    regression_predictions = regression_predictions.reshape(-1, 4)\n",
    "\n",
    "    regression_loss = torch.tensor(0).float()\n",
    "    mask = torch.squeeze(class_targets > 0)\n",
    "    num_foreground = mask.sum().item()\n",
    "    if mask.numel() > 0:\n",
    "        regression_loss = F.smooth_l1_loss(\n",
    "            regression_predictions[mask], regression_targets[mask]\n",
    "        )\n",
    "\n",
    "    mask = torch.squeeze(class_targets > -1)\n",
    "    \n",
    "    # the sum of focal loss terms is normalized by the number\n",
    "    # of anchors assigned to a ground-truth box\n",
    "    classification_loss = softmax_focal_loss(\n",
    "        class_predictions[mask],\n",
    "        class_targets[mask],\n",
    "        alpha=0.25,\n",
    "        gamma=2,\n",
    "        reduction=\"sum\",\n",
    "    ) / num_foreground\n",
    "\n",
    "    return classification_loss, regression_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def compute_detections(classifications, regressions, feature_map_width, anchor_box_step, anchor_box_size):\n",
    "    \"\"\" Compute a set of boxes, class predictions, and foreground scores from\n",
    "        detection model outputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    classifications : torch.Tensor, shape=(N, R*C, # classes)\n",
    "        A set of class predictions at each spatial location.\n",
    "\n",
    "    regressions : torch.Tensor, shape=(N, R*C, 4)\n",
    "        A set of predicted box offsets, in (x, y, w, h) at each spatial location.\n",
    "\n",
    "    feature_map_width : int\n",
    "        The number of pixels in the feature map, along the x direction.\n",
    "\n",
    "    anchor_box_step : int\n",
    "        The number of pixels (in image space) between each anchor box.\n",
    "\n",
    "    anchor_box_size : int\n",
    "        The side length of the anchor box.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[numpy.ndarray shape=(R*C, 4), numpy.ndarray shape=(R*C, 1), numpy.ndarray shape=(R*C,)]\n",
    "        The (boxes, class predictions, foreground scores) at each spatial location.\n",
    "    \"\"\"\n",
    "    box_predictions = np.empty((len(regressions), 4), dtype=np.float32)\n",
    "    scores = torch.softmax(classifications, dim=-1).detach().cpu().numpy()\n",
    "    scores = 1 - scores[:, 0]  # foreground score\n",
    "\n",
    "    class_predictions = classifications.argmax(dim=-1, keepdim=True).detach().cpu().numpy()\n",
    "    regressions = regressions.detach().cpu().numpy()\n",
    "\n",
    "    y, x = np.divmod(np.arange(len(classifications)), feature_map_width, dtype=np.float32)\n",
    "    x_reg, y_reg, w_reg, h_reg = regressions.T  # transform (R*C, 4) to (4, R*C) for assignment\n",
    "    x = anchor_box_step * x + anchor_box_size * x_reg\n",
    "    y = anchor_box_step * y + anchor_box_size * y_reg\n",
    "\n",
    "    half_w = np.clip(np.exp(w_reg), 0, 10**6) * anchor_box_size / 2\n",
    "    half_h = np.clip(np.exp(h_reg), 0, 10**6) * anchor_box_size / 2\n",
    "\n",
    "    box_predictions[:, 0] = x - half_w  # x1\n",
    "    box_predictions[:, 1] = y - half_h  # y1\n",
    "    box_predictions[:, 2] = x + half_w  # x2\n",
    "    box_predictions[:, 3] = y + half_h  # y2\n",
    "\n",
    "    return box_predictions, class_predictions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def compute_batch_stats(class_predictions, regression_predictions, boxes, labels, feature_map_width,\n",
    "                        anchor_box_step=16, anchor_box_size=32, threshold=0.5):\n",
    "    \"\"\" Compute the batch statistics (AP and AR) given a batch of predictions and truth.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    class_predictions : numpy.ndarray, shape=(N, K, C)\n",
    "        The predicted class scores of each of N images at each of K anchor boxes.\n",
    "\n",
    "    regression_predictions : numpy.ndarray, shape=(N, K, 4)\n",
    "        The predicted regression values of each of N images at each of K anchor boxes.\n",
    "\n",
    "    boxes : numpy.ndarray, shape=(N,)\n",
    "        The truth boxes for each image. Note that each of the N elements is of \n",
    "        shape (W_i, 4), where W_i is the number of objects in image i.\n",
    "\n",
    "    labels : numpy.ndarray, shape=(N,)\n",
    "        The truth labels for each image. Note that each of the N elements is of\n",
    "        shape (W_i,), where  W_i is the number of objects in image i.\n",
    "\n",
    "    feature_map_width : int, optional (default=40)\n",
    "        The width of the feature map.\n",
    "\n",
    "    anchor_box_step : int, optional (default=16)\n",
    "        The stride across the image at which anchor boxes are placed.\n",
    "\n",
    "    anchor_box_size : int, optional (default=32)\n",
    "        The side length of each anchor box.\n",
    "\n",
    "    threshold : Real, optional (default=0.5)\n",
    "        The confidence threshold under which to cull predictions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[float], List[float]]\n",
    "        The (aps, ars) for the images.\n",
    "    \"\"\"\n",
    "    aps, ars = [], []\n",
    "    for i in range(len(class_predictions)):\n",
    "        truth_detections = np.hstack((boxes[i], labels[i][:, None]))\n",
    "\n",
    "        box_preds, class_preds, scores = compute_detections(class_predictions[i], \n",
    "                                                            regression_predictions[i],\n",
    "                                                            feature_map_width, \n",
    "                                                            anchor_box_step, \n",
    "                                                            anchor_box_size)\n",
    "\n",
    "        keep_idxs = non_max_suppression(box_preds, scores, 0.3)\n",
    "        detections = np.hstack((box_preds, class_preds))\n",
    "        detections = detections[keep_idxs]\n",
    "        detections = detections[scores[keep_idxs] > threshold]\n",
    "\n",
    "        aps.append(compute_precision(detections, truth_detections, 0.5))\n",
    "        ars.append(compute_recall(detections, truth_detections, 0.5))\n",
    "    return aps, ars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now we can create our model and train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Model().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), weight_decay=5e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(epoch, batch_size, log_file=None, eval_every=10):\n",
    "    torch.set_grad_enabled(True)\n",
    "    model.train()\n",
    "\n",
    "    idxs = torch.randperm(len(train_images))  # shuffle indices\n",
    "    for itr, batch in enumerate(\n",
    "        (slice(i, i + batch_size) for i in range(0, len(train_images), batch_size))\n",
    "    ):\n",
    "        batch = idxs[batch]\n",
    "\n",
    "        class_predictions, regression_predictions = model(train_images[batch])\n",
    "        aps, ars = compute_batch_stats(\n",
    "            class_predictions,\n",
    "            regression_predictions,\n",
    "            train_boxes[batch],\n",
    "            train_labels[batch],\n",
    "            feature_map_width=16,\n",
    "        )\n",
    "        total_cls_loss, total_reg_loss = loss(\n",
    "            class_predictions,\n",
    "            regression_predictions,\n",
    "            train_cls_targs[batch],\n",
    "            train_reg_targs[batch],\n",
    "        )\n",
    "\n",
    "        optim.zero_grad()\n",
    "        (total_cls_loss + total_reg_loss).backward()\n",
    "        optim.step()\n",
    "\n",
    "\n",
    "        if log_file is not None:\n",
    "            log_str = f\"Epoch {epoch:03d} batch {itr+1:05d}: \"\n",
    "            log_str += f\"Loss {(total_cls_loss + total_reg_loss).item():0.4f}\"\n",
    "            log_str += f\" Regression loss {total_reg_loss.item():0.4f}\"\n",
    "            log_str += f\" Classification loss {total_cls_loss.item():0.4f}\"\n",
    "            log_str += f\" AP {np.mean(aps):0.4f} AR {np.mean(ars):0.4f}\"\n",
    "            log_file.write(log_str + \"\\n\")\n",
    "\n",
    "        if (itr + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch} iteration {itr + 1}\", end=\"\\r\")\n",
    "\n",
    "    if epoch % eval_every == 0:\n",
    "        aps, ars = [], []\n",
    "        idxs = torch.arange(len(val_images))\n",
    "        for itr, batch in enumerate(\n",
    "            (slice(i, i + batch_size) for i in range(0, len(val_images), batch_size))\n",
    "        ):\n",
    "            batch = idxs[batch]\n",
    "\n",
    "            class_predictions, regression_predictions = model(val_images[batch])\n",
    "            ap, ar = compute_batch_stats(\n",
    "                class_predictions,\n",
    "                regression_predictions,\n",
    "                val_boxes[batch],\n",
    "                val_labels[batch],\n",
    "                feature_map_width=16,\n",
    "            )\n",
    "            aps += ap\n",
    "            ars += ar\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d}  Validation: {np.mean(aps):0.3f} mAP, {np.mean(ars):0.3f} mAR\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with open('log.txt', 'w') as log:\n",
    "    for epoch in range(1, 101):\n",
    "        train_epoch(epoch, batch_size, log, eval_every=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "At this point, we tend to get around 65% precision and 50% recall. We'll go through a visualization\n",
    "below, but note that you can keep training for longer to get better performance. We've found that\n",
    "going for 500 epochs tends to do pretty well. You can also generate more training samples at the\n",
    "top of the notebook and keep the number of epochs the same. They key is additional training\n",
    "iterations. After 500 epochs we're up to about 75% precision and 75% recall.\n",
    "\n",
    "Now we can visualize the training process and see how our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "losses, aps, ars = [], [], []\n",
    "with open('log.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        losses.append(float(line[5]))\n",
    "        aps.append(float(line[-3]))\n",
    "        ars.append(float(line[-1]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(range(len(aps)), aps)\n",
    "ax.scatter(range(len(ars)), ars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_random_image(model, threshold=0, idx=None):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    idx = np.random.randint(len(val_images)) if idx is None else idx\n",
    "    print(idx)\n",
    "    img = val_images[idx]\n",
    "    ax.imshow(img.permute(1, 2, 0).cpu())\n",
    "    ax.axis('off')\n",
    "\n",
    "    out_cls, out_reg = model(img[np.newaxis])\n",
    "    box_preds, class_preds, scores = compute_detections(out_cls.squeeze(), \n",
    "                                                        out_reg.squeeze(), \n",
    "                                                        feature_map_width=16,\n",
    "                                                        anchor_box_step=16, \n",
    "                                                        anchor_box_size=32)\n",
    "\n",
    "    keep = scores > threshold\n",
    "    box_preds = box_preds[keep]\n",
    "    class_preds = class_preds[keep]\n",
    "    scores = scores[keep]\n",
    "    keep_idxs = non_max_suppression(box_preds, scores, threshold=0.1)\n",
    "    box_preds = box_preds[keep_idxs]\n",
    "    class_preds = class_preds[keep_idxs]\n",
    "\n",
    "    for class_pred, box_pred in zip(class_preds, box_preds):\n",
    "        if class_pred > 0:\n",
    "            x1, y1, x2, y2 = box_pred\n",
    "            ax.add_patch(Rectangle((x1, y1), x2 - x1, y2 - y1, color='r', fill=None, lw=2))\n",
    "            label = int(class_pred)\n",
    "            label = 'rectangle' if label == 1 else label\n",
    "            label = 'triangle' if label == 2 else label\n",
    "            label = 'circle' if label == 3 else label\n",
    "            ax.annotate(label, (x1, y1), color='r', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_image(model, idx=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-scicomp]",
   "language": "python",
   "name": "conda-env-.conda-scicomp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "name": "example-detector.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
